{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comments about RANK</h3>\n",
    "\n",
    "RANK() OVER (ORDER BY Tag) â€“ ranking the table based on Tag values only, same rank for the same Tags across the table\n",
    "\n",
    "RANK() OVER (PARTITION BY Year ORDER BY Tag) - ranking the table based on tags values, ranking restart for each new Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile query.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\n",
    "\n",
    "USE stackoverflow_;\n",
    "\n",
    "WITH tags_2009 as \n",
    "        (SELECT year, tag, count(1) as popularity_tag, RANK() OVER (PARTITION BY year ORDER BY count(1) DESC) as rank_t\n",
    "        FROM posts\n",
    "        LATERAL VIEW explode(tags) lateral_table AS tag\n",
    "        WHERE year = '2009' AND post_type_id = '1'\n",
    "        GROUP BY year, tag),\n",
    "    tags_2016 as \n",
    "        (SELECT year, tag, count(1) as popularity_tag, RANK() OVER (PARTITION BY year ORDER BY count(1) DESC) as rank_t\n",
    "        FROM posts\n",
    "        LATERAL VIEW explode(tags) lateral_table AS tag\n",
    "        WHERE year = '2016' AND post_type_id = '1'\n",
    "        GROUP BY year, tag)\n",
    "SELECT a.tag, b.rank_t, a.rank_t, b.popularity_tag, a.popularity_tag \n",
    "    FROM (SELECT tag, rank_t, popularity_tag from tags_2009) a \n",
    "    JOIN (SELECT tag, rank_t, popularity_tag from tags_2016) b \n",
    "    ON a.tag = b.tag\n",
    "    ORDER BY b.popularity_tag DESC\n",
    "    LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar]\n",
      "OK\n",
      "Time taken: 1.035 seconds\n",
      "Query ID = jovyan_20180523050000_c0ae00f2-6ef7-4b68-bbec-f10fdae542c4\n",
      "Total jobs = 8\n",
      "Launching Job 1 out of 8\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1527028425974_0033, Tracking URL = http://19e0614a425c:8088/proxy/application_1527028425974_0033/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1527028425974_0033\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2018-05-23 05:00:19,871 Stage-1 map = 0%,  reduce = 0%\n",
      "2018-05-23 05:00:28,727 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.57 sec\n",
      "2018-05-23 05:00:38,500 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.92 sec\n",
      "MapReduce Total cumulative CPU time: 10 seconds 920 msec\n",
      "Ended Job = job_1527028425974_0033\n",
      "Launching Job 2 out of 8\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1527028425974_0034, Tracking URL = http://19e0614a425c:8088/proxy/application_1527028425974_0034/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1527028425974_0034\n",
      "Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1\n",
      "2018-05-23 05:00:55,085 Stage-5 map = 0%,  reduce = 0%\n",
      "2018-05-23 05:01:04,749 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 7.33 sec\n",
      "2018-05-23 05:01:13,382 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 12.26 sec\n",
      "MapReduce Total cumulative CPU time: 12 seconds 260 msec\n",
      "Ended Job = job_1527028425974_0034\n",
      "Launching Job 3 out of 8\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1527028425974_0035, Tracking URL = http://19e0614a425c:8088/proxy/application_1527028425974_0035/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1527028425974_0035\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2018-05-23 05:01:31,027 Stage-2 map = 0%,  reduce = 0%\n",
      "2018-05-23 05:01:38,562 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.15 sec\n",
      "2018-05-23 05:01:48,227 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 9.03 sec\n",
      "MapReduce Total cumulative CPU time: 9 seconds 30 msec\n",
      "Ended Job = job_1527028425974_0035\n",
      "Launching Job 4 out of 8\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1527028425974_0036, Tracking URL = http://19e0614a425c:8088/proxy/application_1527028425974_0036/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1527028425974_0036\n",
      "Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1\n",
      "2018-05-23 05:02:05,459 Stage-6 map = 0%,  reduce = 0%\n",
      "2018-05-23 05:02:14,048 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 4.51 sec\n",
      "2018-05-23 05:02:23,675 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 11.43 sec\n",
      "MapReduce Total cumulative CPU time: 11 seconds 430 msec\n",
      "Ended Job = job_1527028425974_0036\n",
      "Stage-10 is filtered out by condition resolver.\n",
      "Stage-11 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Execution log at: /tmp/jovyan/jovyan_20180523050000_c0ae00f2-6ef7-4b68-bbec-f10fdae542c4.log\n",
      "2018-05-23 05:02:30\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2018-05-23 05:02:31\tDump the side-table for tag: 0 with group count: 2369 into file: file:/tmp/jovyan/3a0a207c-7ec2-4b3e-9d4d-2c9f24a0939f/hive_2018-05-23_05-00-02_240_7144927244564598577-1/-local-10010/HashTable-Stage-8/MapJoin-mapfile10--.hashtable\n",
      "2018-05-23 05:02:32\tUploaded 1 File to: file:/tmp/jovyan/3a0a207c-7ec2-4b3e-9d4d-2c9f24a0939f/hive_2018-05-23_05-00-02_240_7144927244564598577-1/-local-10010/HashTable-Stage-8/MapJoin-mapfile10--.hashtable (76823 bytes)\n",
      "2018-05-23 05:02:32\tEnd of local task; Time Taken: 1.645 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 6 out of 8\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1527028425974_0037, Tracking URL = http://19e0614a425c:8088/proxy/application_1527028425974_0037/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1527028425974_0037\n",
      "Hadoop job information for Stage-8: number of mappers: 1; number of reducers: 0\n",
      "2018-05-23 05:02:44,378 Stage-8 map = 0%,  reduce = 0%\n",
      "2018-05-23 05:02:52,927 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 3.66 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 660 msec\n",
      "Ended Job = job_1527028425974_0037\n",
      "Launching Job 7 out of 8\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1527028425974_0038, Tracking URL = http://19e0614a425c:8088/proxy/application_1527028425974_0038/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1527028425974_0038\n",
      "Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1\n",
      "2018-05-23 05:03:10,145 Stage-4 map = 0%,  reduce = 0%\n",
      "2018-05-23 05:03:17,715 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 2.95 sec\n",
      "2018-05-23 05:03:26,213 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 6.22 sec\n",
      "MapReduce Total cumulative CPU time: 6 seconds 220 msec\n",
      "Ended Job = job_1527028425974_0038\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 10.92 sec   HDFS Read: 145464 HDFS Write: 79828 SUCCESS\n",
      "Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 12.26 sec   HDFS Read: 835398 HDFS Write: 331434 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 9.03 sec   HDFS Read: 86118 HDFS Write: 74637 SUCCESS\n",
      "Stage-Stage-6: Map: 1  Reduce: 1   Cumulative CPU: 11.43 sec   HDFS Read: 337724 HDFS Write: 311766 SUCCESS\n",
      "Stage-Stage-8: Map: 1   Cumulative CPU: 3.66 sec   HDFS Read: 316315 HDFS Write: 55844 SUCCESS\n",
      "Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 6.22 sec   HDFS Read: 61036 HDFS Write: 188 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 53 seconds 520 msec\n",
      "OK\n",
      "javascript\t1\t5\t2771\t192\n",
      "java\t2\t2\t2033\t243\n",
      "android\t3\t52\t1809\t25\n",
      "php\t4\t3\t1673\t215\n",
      "python\t5\t11\t1585\t108\n",
      "c#\t6\t1\t1519\t423\n",
      "html\t7\t14\t1212\t84\n",
      "jquery\t8\t8\t1167\t141\n",
      "ios\t9\t186\t914\t7\n",
      "css\t10\t20\t801\t59\n",
      "Time taken: 206.163 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "! hive -f query.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
